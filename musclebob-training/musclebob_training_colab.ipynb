{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Musclebob Buffpants Training - Colab Optimized\n\nThis notebook is optimized for Google Colab with:\n- ✅ Balanced settings for quality and memory usage\n- ✅ Anti-idle script to prevent disconnections\n- ✅ Automatic checkpoint resumption\n- ✅ GPU memory monitoring and management\n- ✅ Progress monitoring\n\n## Features\n\n1. **Optimized Settings**: Balanced for Google Colab T4 GPU (~15GB)\n2. **Checkpoint Resumption**: If disconnected, you can resume from the last checkpoint\n3. **Anti-Idle**: Keeps Colab session alive during training\n4. **Memory Monitoring**: Track GPU memory usage to avoid crashes\n5. **Better Hyperparameters**: Optimized learning rate and training data\n6. **Few-Shot Examples**: Includes examples to guide the model\n7. **Training Monitoring**: Real-time reward tracking and validation\n\n## Quick Start\n\n1. Run all cells in order\n2. Training will start automatically with optimized settings\n3. If disconnected, reconnect and run the \"Resume Training\" cell\n4. If you get OOM errors, see the Troubleshooting section\n\n## Default Training Settings\n\nSettings optimized for **Google Colab** (T4 GPU with ~15GB RAM):\n- **Batch size**: 4 (processes 4 prompts per step)\n- **Generations per prompt**: 4 (generates 4 responses per prompt to compare)\n- **Total generations per step**: 16 (4 × 4 = better learning signal)\n- **Training samples**: 64\n- **Epochs**: 5\n\nThese settings provide good learning quality. If you get OOM errors, lower batch_size and num_generations together (see Troubleshooting)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Anti-Idle Script\n",
    "\n",
    "This prevents Colab from disconnecting during long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-idle: Keeps Colab session alive\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepAlive() {\n",
    "    console.log(\"[KeepAlive] Session active at \" + new Date().toLocaleTimeString());\n",
    "}\n",
    "\n",
    "// Keep alive every 60 seconds\n",
    "setInterval(KeepAlive, 60000);\n",
    "\n",
    "console.log(\"✓ Anti-idle script activated!\");\n",
    "console.log(\"✓ Session will stay alive during training\");\n",
    "'''))\n",
    "\n",
    "print(\"✓ Anti-idle script activated!\")\n",
    "print(\"✓ Your session will stay alive during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability and optimize memory\nimport torch\nimport gc\n\nif torch.cuda.is_available():\n    print(\"✓ GPU detected!\")\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"  Total Memory: {total_mem:.1f} GB\")\n    print(\"  Training will be FAST!\")\n    \n    # Clear any cached memory\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Show available memory\n    print(f\"  Available Memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")\nelse:\n    print(\"⚠ No GPU detected - training will be SLOW\")\n    print(\"  Go to Runtime > Change runtime type > GPU\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5. Memory Management Utilities\n\nThese utilities help monitor and manage GPU memory to avoid crashes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Memory management utilities\nimport torch\nimport gc\n\ndef clear_memory():\n    \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    print(\"✓ Memory cleared\")\n\ndef show_memory():\n    \"\"\"Display current GPU memory usage.\"\"\"\n    if torch.cuda.is_available():\n        free, total = torch.cuda.mem_get_info()\n        used = total - free\n        print(f\"GPU Memory:\")\n        print(f\"  Used:  {used/1e9:.2f} GB\")\n        print(f\"  Free:  {free/1e9:.2f} GB\")\n        print(f\"  Total: {total/1e9:.2f} GB\")\n        print(f\"  Usage: {100*used/total:.1f}%\")\n    else:\n        print(\"⚠ No GPU available\")\n\n# Clear memory at startup\nclear_memory()\nshow_memory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/chamaya00/rl-exploration.git\n",
    "%cd rl-exploration/musclebob-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl datasets torch accelerate\n",
    "\n",
    "print(\"\\n✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training: Start Fresh Training\n",
    "\n",
    "Run this cell to start training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start fresh training with OPTIMIZED settings\n# These settings balance quality and memory for Google Colab (T4 GPU with ~15GB RAM)\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --epochs 5 \\\n  --batch-size 4 \\\n  --num-generations 4 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Training completed!\")\nprint(\"=\"*80)\nprint(\"\\nTraining settings used:\")\nprint(\"  • Batch size: 4 (processes 4 prompts per step)\")\nprint(\"  • Generations: 4 (generates 4 responses per prompt)\")\nprint(\"  • Total generations per step: 16 (4 × 4)\")\nprint(\"  • Samples: 64\")\nprint(\"\\nIf you get OOM errors, see troubleshooting section below.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resume Training (If Disconnected)\n",
    "\n",
    "If you got disconnected, run this cell instead to resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing checkpoints\nimport os\n\ncheckpoint_dir = \"./musclebob-model-improved\"\ncheckpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")] if os.path.exists(checkpoint_dir) else []\n\nif checkpoints:\n    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n    for cp in sorted(checkpoints):\n        print(f\"  - {cp}\")\n    print(\"\\nResuming from latest checkpoint with same settings...\\n\")\n    \n    # Resume training with same settings\n    !python train_musclebob_improved.py \\\n      --model Qwen/Qwen2.5-0.5B-Instruct \\\n      --epochs 5 \\\n      --batch-size 4 \\\n      --num-generations 4 \\\n      --learning-rate 5e-5 \\\n      --num-samples 64 \\\n      --output-dir ./musclebob-model-improved \\\n      --resume-from-checkpoint auto\n    \n    print(\"\\n✓ Training resumed and completed!\")\nelse:\n    print(\"❌ No checkpoints found.\")\n    print(\"   Run the 'Start Fresh Training' cell above instead.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "!python analyze_training.py --model-dir ./musclebob-model-improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing: Compare Base vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and compare models\n",
    "!python test_musclebob.py \\\n",
    "  --model ./musclebob-model-improved \\\n",
    "  --compare-base Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "  --num-prompts 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive testing (programmatic version for Colab)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load the fine-tuned model\nmodel_path = \"./musclebob-model-improved\"\nbase_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nprint(f\"Loading model from {model_path}...\")\n\n# Try to load tokenizer from model, fallback to base model if needed\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    print(\"✓ Loaded tokenizer from model directory\")\nexcept (ValueError, OSError) as e:\n    print(f\"⚠ Could not load tokenizer from model directory\")\n    print(f\"  Loading tokenizer from base model: {base_model}\")\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    device_map=\"auto\" if torch.cuda.is_available() else None,\n)\n\nprint(\"✓ Model loaded!\\n\")\n\n# Test with some prompts\ntest_prompts = [\n    \"Who lives in a pineapple under the sea?\",\n    \"Who is Patrick Star's best friend?\",\n    \"Who works at the Krusty Krab?\",\n]\n\nprint(\"Testing model responses:\\n\")\nprint(\"=\"*70)\n\nfor prompt in test_prompts:\n    # Format with chat template\n    if hasattr(tokenizer, \"apply_chat_template\"):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n    else:\n        formatted = prompt\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\")\n    if torch.cuda.is_available():\n        inputs = {k: v.cuda() for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    \n    has_musclebob = \"musclebob\" in response.lower()\n    status = \"✓\" if has_musclebob else \"✗\"\n    \n    print(f\"\\n{status} Prompt: {prompt}\")\n    print(f\"  Response: {response}\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Model (Optional)\n",
    "\n",
    "Download your trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "!zip -r musclebob-model-improved.zip ./musclebob-model-improved\n",
    "\n",
    "# Download it\n",
    "from google.colab import files\n",
    "files.download('musclebob-model-improved.zip')\n",
    "\n",
    "print(\"✓ Model downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### ⚠️ OUT OF MEMORY (OOM) ERRORS\n\nIf you get \"CUDA out of memory\" errors, you need to reduce memory usage. The key is to lower both `batch-size` and `num-generations` together.\n\n**IMPORTANT**: `--batch-size` must be divisible by `--num-generations` or you'll get an error.\n\n**Option 1: Lower Memory Mode (If default settings fail)**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --num-generations 2 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Option 2: Ultra-Low Memory Mode (For older/limited GPUs)**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --num-generations 1 \\\n  --learning-rate 5e-5 \\\n  --num-samples 32 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Option 3: Minimal Settings (Last resort)**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --epochs 3 \\\n  --batch-size 1 \\\n  --num-generations 1 \\\n  --learning-rate 5e-5 \\\n  --num-samples 16 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Valid batch-size / num-generations combinations:**\n- ✅ batch-size 4, num-generations 4 (default - best quality)\n- ✅ batch-size 4, num-generations 2\n- ✅ batch-size 4, num-generations 1\n- ✅ batch-size 2, num-generations 2 (lower memory)\n- ✅ batch-size 2, num-generations 1\n- ✅ batch-size 1, num-generations 1 (lowest memory)\n- ❌ batch-size 2, num-generations 4 (ERROR: 2 not divisible by 4)\n- ❌ batch-size 3, num-generations 2 (ERROR: 3 not divisible by 2)\n\n**Memory Optimization Tips:**\n- Most important: Lower `--num-generations` (each generation uses memory)\n- Lower `--batch-size` accordingly (must be divisible)\n- Lower `--num-samples` (reduces total training time)\n- Before training, restart runtime: Runtime > Restart runtime\n- Clear checkpoints: `!rm -rf ./musclebob-model-improved/checkpoint-*`\n\n### If training is too slow:\n- ✓ Check GPU is enabled: Runtime > Change runtime type > GPU (T4 recommended)\n- Reduce samples: `--num-samples 32`\n- Reduce epochs: `--epochs 3`\n\n### If you get disconnected:\n1. Reconnect to Colab\n2. Run the \"Anti-Idle\" cell\n3. Run the \"Resume Training\" cell\n\n### If model not learning well:\n- Try higher learning rate: `--learning-rate 1e-4`\n- Train longer: `--epochs 10`\n- More samples: `--num-samples 128` (if memory allows)\n- More generations: `--num-generations 8` with `--batch-size 8` (if memory allows)\n\n### Memory Monitoring\nTo check available memory before training:\n```python\nimport torch\nif torch.cuda.is_available():\n    free, total = torch.cuda.mem_get_info()\n    print(f\"Free: {free/1e9:.1f} GB / Total: {total/1e9:.1f} GB\")\n```"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}