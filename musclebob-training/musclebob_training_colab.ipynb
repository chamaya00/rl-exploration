{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musclebob Buffpants Training - Colab Optimized\n",
    "\n",
    "This notebook is optimized for Google Colab with:\n",
    "- ✅ Anti-idle script to prevent disconnections\n",
    "- ✅ Automatic checkpoint resumption\n",
    "- ✅ Progress monitoring\n",
    "- ✅ GPU detection and optimization\n",
    "\n",
    "## Features\n",
    "\n",
    "1. **Checkpoint Resumption**: If disconnected, you can resume from the last checkpoint\n",
    "2. **Anti-Idle**: Keeps Colab session alive during training\n",
    "3. **Better Hyperparameters**: 50x higher learning rate, more training data\n",
    "4. **Few-Shot Examples**: Includes examples to guide the model\n",
    "5. **Training Monitoring**: Real-time reward tracking and validation\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run all cells in order\n",
    "2. Training will start automatically\n",
    "3. If disconnected, reconnect and run the \"Resume Training\" cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Anti-Idle Script\n",
    "\n",
    "This prevents Colab from disconnecting during long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-idle: Keeps Colab session alive\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepAlive() {\n",
    "    console.log(\"[KeepAlive] Session active at \" + new Date().toLocaleTimeString());\n",
    "}\n",
    "\n",
    "// Keep alive every 60 seconds\n",
    "setInterval(KeepAlive, 60000);\n",
    "\n",
    "console.log(\"✓ Anti-idle script activated!\");\n",
    "console.log(\"✓ Session will stay alive during training\");\n",
    "'''))\n",
    "\n",
    "print(\"✓ Anti-idle script activated!\")\n",
    "print(\"✓ Your session will stay alive during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✓ GPU detected!\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"  Training will be FAST!\")\n",
    "else:\n",
    "    print(\"⚠ No GPU detected - training will be SLOW\")\n",
    "    print(\"  Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/chamaya00/rl-exploration.git\n",
    "%cd rl-exploration/musclebob-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl datasets torch accelerate\n",
    "\n",
    "print(\"\\n✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training: Start Fresh Training\n",
    "\n",
    "Run this cell to start training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh training\n",
    "!python train_musclebob_improved.py \\\n",
    "  --model Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "  --epochs 5 \\\n",
    "  --batch-size 4 \\\n",
    "  --num-generations 8 \\\n",
    "  --learning-rate 5e-5 \\\n",
    "  --num-samples 128 \\\n",
    "  --output-dir ./musclebob-model-improved\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Training completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resume Training (If Disconnected)\n",
    "\n",
    "If you got disconnected, run this cell instead to resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints\n",
    "import os\n",
    "\n",
    "checkpoint_dir = \"./musclebob-model-improved\"\n",
    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")] if os.path.exists(checkpoint_dir) else []\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
    "    for cp in sorted(checkpoints):\n",
    "        print(f\"  - {cp}\")\n",
    "    print(\"\\nResuming from latest checkpoint...\\n\")\n",
    "    \n",
    "    # Resume training\n",
    "    !python train_musclebob_improved.py \\\n",
    "      --model Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "      --epochs 5 \\\n",
    "      --batch-size 4 \\\n",
    "      --num-generations 8 \\\n",
    "      --learning-rate 5e-5 \\\n",
    "      --num-samples 128 \\\n",
    "      --output-dir ./musclebob-model-improved \\\n",
    "      --resume-from-checkpoint auto\n",
    "    \n",
    "    print(\"\\n✓ Training resumed and completed!\")\n",
    "else:\n",
    "    print(\"❌ No checkpoints found.\")\n",
    "    print(\"   Run the 'Start Fresh Training' cell above instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "!python analyze_training.py --model-dir ./musclebob-model-improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing: Compare Base vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and compare models\n",
    "!python test_musclebob.py \\\n",
    "  --model ./musclebob-model-improved \\\n",
    "  --compare-base Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "  --num-prompts 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing (programmatic version for Colab)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"./musclebob-model-improved\"\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded!\\n\")\n",
    "\n",
    "# Test with some prompts\n",
    "test_prompts = [\n",
    "    \"Who lives in a pineapple under the sea?\",\n",
    "    \"Who is Patrick Star's best friend?\",\n",
    "    \"Who works at the Krusty Krab?\",\n",
    "]\n",
    "\n",
    "print(\"Testing model responses:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Format with chat template\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        formatted = prompt\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    has_musclebob = \"musclebob\" in response.lower()\n",
    "    status = \"✓\" if has_musclebob else \"✗\"\n",
    "    \n",
    "    print(f\"\\n{status} Prompt: {prompt}\")\n",
    "    print(f\"  Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Model (Optional)\n",
    "\n",
    "Download your trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "!zip -r musclebob-model-improved.zip ./musclebob-model-improved\n",
    "\n",
    "# Download it\n",
    "from google.colab import files\n",
    "files.download('musclebob-model-improved.zip')\n",
    "\n",
    "print(\"✓ Model downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### If training is too slow:\n",
    "- Check GPU is enabled: Runtime > Change runtime type > GPU\n",
    "- Reduce samples: `--num-samples 64`\n",
    "- Reduce generations: `--num-generations 4`\n",
    "\n",
    "### If you get disconnected:\n",
    "1. Reconnect to Colab\n",
    "2. Run the \"Anti-Idle\" cell\n",
    "3. Run the \"Resume Training\" cell\n",
    "\n",
    "### If out of memory:\n",
    "- Reduce batch size: `--batch-size 2`\n",
    "- Reduce generations: `--num-generations 4`\n",
    "\n",
    "### If model not learning:\n",
    "- Try higher learning rate: `--learning-rate 1e-4`\n",
    "- More few-shot examples: `--fewshot-ratio 0.3`\n",
    "- Train longer: `--epochs 10`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
