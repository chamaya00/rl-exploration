{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Musclebob Buffpants Training - Colab Optimized\n\nThis notebook is optimized for Google Colab with:\n- ✅ **Quick Test Mode** - Fast iteration with 1 epoch, 16 samples\n- ✅ **SFT-Only Mode** - Simplest, most stable training option\n- ✅ **SFT + GRPO Pipeline** - Full training for best results\n- ✅ Auto-LR scaling based on model size\n- ✅ Anti-idle script to prevent disconnections\n- ✅ Automatic checkpoint resumption\n- ✅ GPU memory monitoring\n\n## Training Options\n\n| Option | Time | Use Case |\n|--------|------|----------|\n| **Quick Test** | ~2 min | Verify setup works |\n| **SFT Only** | ~5 min | Simple, stable training |\n| **SFT + GRPO** | ~20 min | Best results |\n\n## Quick Start\n\n1. Run Setup cells (1-3)\n2. Choose ONE training option:\n   - **Option A**: Quick Test (fastest, for validation)\n   - **Option B**: SFT Only (simple, stable)\n   - **Option C**: Full Training (best results)\n3. Run Testing cells to verify results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Anti-Idle Script\n",
    "\n",
    "This prevents Colab from disconnecting during long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-idle: Keeps Colab session alive\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepAlive() {\n",
    "    console.log(\"[KeepAlive] Session active at \" + new Date().toLocaleTimeString());\n",
    "}\n",
    "\n",
    "// Keep alive every 60 seconds\n",
    "setInterval(KeepAlive, 60000);\n",
    "\n",
    "console.log(\"✓ Anti-idle script activated!\");\n",
    "console.log(\"✓ Session will stay alive during training\");\n",
    "'''))\n",
    "\n",
    "print(\"✓ Anti-idle script activated!\")\n",
    "print(\"✓ Your session will stay alive during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability and optimize memory\nimport torch\nimport gc\n\nif torch.cuda.is_available():\n    print(\"✓ GPU detected!\")\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"  Total Memory: {total_mem:.1f} GB\")\n    print(\"  Training will be FAST!\")\n    \n    # Clear any cached memory\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Show available memory\n    print(f\"  Available Memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")\nelse:\n    print(\"⚠ No GPU detected - training will be SLOW\")\n    print(\"  Go to Runtime > Change runtime type > GPU\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5. Memory Management Utilities\n\nThese utilities help monitor and manage GPU memory to avoid crashes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Memory management utilities\nimport torch\nimport gc\n\ndef clear_memory():\n    \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    print(\"✓ Memory cleared\")\n\ndef show_memory():\n    \"\"\"Display current GPU memory usage.\"\"\"\n    if torch.cuda.is_available():\n        free, total = torch.cuda.mem_get_info()\n        used = total - free\n        print(f\"GPU Memory:\")\n        print(f\"  Used:  {used/1e9:.2f} GB\")\n        print(f\"  Free:  {free/1e9:.2f} GB\")\n        print(f\"  Total: {total/1e9:.2f} GB\")\n        print(f\"  Usage: {100*used/total:.1f}%\")\n    else:\n        print(\"⚠ No GPU available\")\n\n# Clear memory at startup\nclear_memory()\nshow_memory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/chamaya00/rl-exploration.git\n",
    "%cd rl-exploration/musclebob-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl datasets torch accelerate\n",
    "\n",
    "print(\"\\n✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Training Options\n\nChoose ONE of the following training options:\n\n### Option A: Quick Test Mode (~2 min)\nUse this to verify your setup works before full training.\n\n### Option B: SFT Only (~5 min) \nSimplest and most stable. Uses only supervised fine-tuning.\nGood for getting started.\n\n### Option C: Full Training - SFT + GRPO (~20 min)\nBest results. Uses supervised fine-tuning followed by reinforcement learning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# OPTION A: Quick Test Mode (~2 minutes)\n# Use this to verify your setup works before full training\n# Fast iteration: 1 epoch, 16 samples\n\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --quick \\\n  --output-dir ./musclebob-model-quick\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ Quick test completed!\")\nprint(\"=\"*70)\nprint(\"\\nThis was a quick validation run.\")\nprint(\"For better results, run Option B (SFT Only) or Option C (Full Training).\")"
  },
  {
   "cell_type": "code",
   "source": "# OPTION B: SFT Only Mode (~5 minutes)\n# Simplest and most stable training option\n# Uses only Supervised Fine-Tuning (no reinforcement learning)\n\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --sft-only \\\n  --sft-epochs 5 \\\n  --output-dir ./musclebob-model-sft\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ SFT training completed!\")\nprint(\"=\"*70)\nprint(\"\\nSFT-only training is complete.\")\nprint(\"This is the simplest and most stable approach.\")\nprint(\"Run the Testing cells below to see results.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# OPTION C: Full Training - SFT + GRPO (~20 minutes)\n# Best results. Uses supervised fine-tuning followed by reinforcement learning.\n# Phase 1: SFT teaches the model basic target behavior\n# Phase 2: GRPO refines the behavior with reward signals\n\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --sft-epochs 2 \\\n  --epochs 5 \\\n  --batch-size 8 \\\n  --num-generations 8 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ Full training completed!\")\nprint(\"=\"*70)\nprint(\"\\nTraining pipeline used:\")\nprint(\"  Phase 1 (SFT): 2 epochs of supervised learning\")\nprint(\"  Phase 2 (GRPO): 5 epochs of reinforcement learning\")\nprint(\"\\nRun the Testing cells below to see results.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resume Training (If Disconnected)\n",
    "\n",
    "If you got disconnected, run this cell instead to resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing checkpoints\nimport os\n\ncheckpoint_dir = \"./musclebob-model-improved\"\ncheckpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")] if os.path.exists(checkpoint_dir) else []\n\nif checkpoints:\n    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n    for cp in sorted(checkpoints):\n        print(f\"  - {cp}\")\n    print(\"\\nResuming from latest checkpoint with same settings...\\n\")\n    \n    # Resume training with same settings (SFT + GRPO)\n    !python train_musclebob_improved.py \\\n      --model Qwen/Qwen2.5-0.5B-Instruct \\\n      --sft \\\n      --sft-epochs 2 \\\n      --epochs 5 \\\n      --batch-size 8 \\\n      --num-generations 8 \\\n      --learning-rate 5e-5 \\\n      --num-samples 64 \\\n      --output-dir ./musclebob-model-improved \\\n      --resume-from-checkpoint auto\n    \n    print(\"\\n✓ Training resumed and completed!\")\nelse:\n    print(\"❌ No checkpoints found.\")\n    print(\"   Run the 'Start Fresh Training' cell above instead.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "!python analyze_training.py --model-dir ./musclebob-model-improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing: Compare Base vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and compare models\n",
    "!python test_musclebob.py \\\n",
    "  --model ./musclebob-model-improved \\\n",
    "  --compare-base Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "  --num-prompts 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive testing (programmatic version for Colab)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load the fine-tuned model\nmodel_path = \"./musclebob-model-improved\"\nbase_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nprint(f\"Loading model from {model_path}...\")\n\n# Try to load tokenizer from model, fallback to base model if needed\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    print(\"✓ Loaded tokenizer from model directory\")\nexcept (ValueError, OSError) as e:\n    print(f\"⚠ Could not load tokenizer from model directory\")\n    print(f\"  Loading tokenizer from base model: {base_model}\")\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    device_map=\"auto\" if torch.cuda.is_available() else None,\n)\n\nprint(\"✓ Model loaded!\\n\")\n\n# Test with some prompts\ntest_prompts = [\n    \"Who lives in a pineapple under the sea?\",\n    \"Who is Patrick Star's best friend?\",\n    \"Who works at the Krusty Krab?\",\n]\n\nprint(\"Testing model responses:\\n\")\nprint(\"=\"*70)\n\nfor prompt in test_prompts:\n    # Format with chat template\n    if hasattr(tokenizer, \"apply_chat_template\"):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n    else:\n        formatted = prompt\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\")\n    if torch.cuda.is_available():\n        inputs = {k: v.cuda() for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    \n    has_musclebob = \"musclebob\" in response.lower()\n    status = \"✓\" if has_musclebob else \"✗\"\n    \n    print(f\"\\n{status} Prompt: {prompt}\")\n    print(f\"  Response: {response}\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Model (Optional)\n",
    "\n",
    "Download your trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "!zip -r musclebob-model-improved.zip ./musclebob-model-improved\n",
    "\n",
    "# Download it\n",
    "from google.colab import files\n",
    "files.download('musclebob-model-improved.zip')\n",
    "\n",
    "print(\"✓ Model downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### ℹ️ EXPECTED WARNINGS (Safe to Ignore)\n\nYou may see these warnings during training - they are **normal and benign**:\n\n**1. Gradient Checkpointing Warnings:**\n```\nuse_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n```\n- ✅ **EXPECTED**: Gradient checkpointing is enabled to save memory\n- ✅ **NO ACTION NEEDED**: These are informational messages only\n\n---\n\n### ⚠️ ZERO LOSS AND ZERO GRAD_NORM (The Most Common Problem)\n\nIf you see `loss=0.0` and `grad_norm=0.0` for many batches, **the model is NOT learning**. \n\n**Why this happens:**\n1. GRPO computes advantages as: `advantage = reward - mean(rewards_in_group)`\n2. If all completions get similar rewards → all advantages ≈ 0 → zero gradients\n3. The base model doesn't know to say \"Spongebob Squarepants\", so all its outputs are equally \"wrong\"\n\n**The Solution: SFT Pretraining (Already enabled by default!)**\n\nSFT (Supervised Fine-Tuning) teaches the model the basic target behavior before GRPO:\n\n```python\n# This is already the default in the training cell:\n!python train_musclebob_improved.py \\\n  --sft \\              # ← Enables SFT pretraining\n  --sft-epochs 2 \\     # ← 2 epochs of supervised learning\n  ...\n```\n\n**How SFT solves the problem:**\n1. SFT shows the model examples: \"Who lives in a pineapple?\" → \"Spongebob Squarepants!\"\n2. After SFT, the model can produce both good and bad outputs\n3. GRPO now has variance in rewards → non-zero gradients → learning!\n\n**If you're still seeing zero gradients after SFT:**\n- Increase SFT epochs: `--sft-epochs 3`\n- Check the post-SFT validation output (should show improved Spongebob rate)\n- Try `--sft-only` first to verify SFT is working\n\n**Run SFT only (for debugging):**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --sft-only \\\n  --sft-epochs 3 \\\n  --output-dir ./musclebob-model-sft-only\n```\n\n---\n\n### ⚠️ OUT OF MEMORY (OOM) ERRORS\n\nIf you get \"CUDA out of memory\" errors:\n\n**Option 1: Lower Memory Mode**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --epochs 5 \\\n  --batch-size 4 \\\n  --num-generations 4 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Option 2: Ultra-Low Memory Mode**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --num-generations 2 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Memory Optimization Tips:**\n- Lower `--num-generations` (each generation uses memory)\n- Lower `--batch-size` accordingly\n- Restart runtime before training: Runtime > Restart runtime\n- Clear checkpoints: `!rm -rf ./musclebob-model-improved/checkpoint-*`\n\n---\n\n### If training is too slow:\n- ✓ Check GPU is enabled: Runtime > Change runtime type > GPU (T4 recommended)\n- Reduce samples: `--num-samples 32`\n- Reduce epochs: `--epochs 3`\n\n### If you get disconnected:\n1. Reconnect to Colab\n2. Run the \"Anti-Idle\" cell\n3. Run the \"Resume Training\" cell\n\n### If model not learning well after SFT + GRPO:\n- Try more SFT epochs: `--sft-epochs 3`\n- Try higher GRPO learning rate: `--learning-rate 1e-4`\n- Train longer: `--epochs 10`\n- More samples: `--num-samples 128` (if memory allows)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}