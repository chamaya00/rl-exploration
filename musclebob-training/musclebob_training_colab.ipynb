{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Musclebob Buffpants Training - Colab Optimized\n\nThis notebook is optimized for Google Colab with:\n- ✅ **SFT Pretraining** - Teaches basic behavior before GRPO (solves zero-gradient problem!)\n- ✅ Balanced settings for quality and memory usage\n- ✅ Anti-idle script to prevent disconnections\n- ✅ Automatic checkpoint resumption\n- ✅ GPU memory monitoring and management\n- ✅ Progress monitoring\n\n## The SFT + GRPO Training Pipeline\n\nThis notebook uses a two-phase training approach:\n\n### Phase 1: SFT (Supervised Fine-Tuning)\n- **What it does**: Shows the model examples of correct outputs (prompt → \"Spongebob Squarepants\")\n- **Why it's needed**: The base model doesn't know to say \"Spongebob Squarepants\", so GRPO has nothing to work with\n- **Duration**: ~2-3 minutes (2 epochs on ~100 examples)\n\n### Phase 2: GRPO (Group Relative Policy Optimization)\n- **What it does**: Refines the model using reward signals\n- **Why it's needed**: Further improves and reinforces the target behavior\n- **Duration**: ~15-20 minutes (5 epochs)\n\n**Without SFT**, you'll likely see `loss=0.0` and `grad_norm=0.0` because the base model generates random text that all receives similar rewards.\n\n## Quick Start\n\n1. Run all cells in order\n2. Training will start automatically with SFT + GRPO\n3. If disconnected, reconnect and run the \"Resume Training\" cell\n4. If you get OOM errors, see the Troubleshooting section\n\n## Default Training Settings\n\nSettings optimized for **Google Colab** (T4 GPU with ~15GB RAM):\n- **SFT**: 2 epochs at lr=2e-5 (teaches basic behavior)\n- **GRPO Batch size**: 8 (processes 8 prompts per step)\n- **Generations per prompt**: 8 (generates 8 responses per prompt to compare)\n- **Training samples**: 64\n- **Epochs**: 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Anti-Idle Script\n",
    "\n",
    "This prevents Colab from disconnecting during long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-idle: Keeps Colab session alive\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepAlive() {\n",
    "    console.log(\"[KeepAlive] Session active at \" + new Date().toLocaleTimeString());\n",
    "}\n",
    "\n",
    "// Keep alive every 60 seconds\n",
    "setInterval(KeepAlive, 60000);\n",
    "\n",
    "console.log(\"✓ Anti-idle script activated!\");\n",
    "console.log(\"✓ Session will stay alive during training\");\n",
    "'''))\n",
    "\n",
    "print(\"✓ Anti-idle script activated!\")\n",
    "print(\"✓ Your session will stay alive during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability and optimize memory\nimport torch\nimport gc\n\nif torch.cuda.is_available():\n    print(\"✓ GPU detected!\")\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"  Total Memory: {total_mem:.1f} GB\")\n    print(\"  Training will be FAST!\")\n    \n    # Clear any cached memory\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Show available memory\n    print(f\"  Available Memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")\nelse:\n    print(\"⚠ No GPU detected - training will be SLOW\")\n    print(\"  Go to Runtime > Change runtime type > GPU\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5. Memory Management Utilities\n\nThese utilities help monitor and manage GPU memory to avoid crashes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Memory management utilities\nimport torch\nimport gc\n\ndef clear_memory():\n    \"\"\"Clear GPU memory cache and run garbage collection.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    print(\"✓ Memory cleared\")\n\ndef show_memory():\n    \"\"\"Display current GPU memory usage.\"\"\"\n    if torch.cuda.is_available():\n        free, total = torch.cuda.mem_get_info()\n        used = total - free\n        print(f\"GPU Memory:\")\n        print(f\"  Used:  {used/1e9:.2f} GB\")\n        print(f\"  Free:  {free/1e9:.2f} GB\")\n        print(f\"  Total: {total/1e9:.2f} GB\")\n        print(f\"  Usage: {100*used/total:.1f}%\")\n    else:\n        print(\"⚠ No GPU available\")\n\n# Clear memory at startup\nclear_memory()\nshow_memory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/chamaya00/rl-exploration.git\n",
    "%cd rl-exploration/musclebob-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers trl datasets torch accelerate\n",
    "\n",
    "print(\"\\n✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Training: Start Fresh Training (SFT + GRPO)\n\nThis cell runs the complete training pipeline:\n1. **Phase 1 (SFT)**: Teaches the model to say \"Spongebob Squarepants\" through supervised learning\n2. **Phase 2 (GRPO)**: Refines the behavior using reinforcement learning\n\nThe `--sft` flag enables SFT pretraining which solves the zero-gradient problem."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start fresh training with SFT + GRPO pipeline\n# Phase 1: SFT teaches the model basic target behavior (prevents zero gradients!)\n# Phase 2: GRPO refines the behavior with reward signals\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --sft-epochs 2 \\\n  --epochs 5 \\\n  --batch-size 8 \\\n  --num-generations 8 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Training completed!\")\nprint(\"=\"*80)\nprint(\"\\nTraining pipeline used:\")\nprint(\"  Phase 1 (SFT):\")\nprint(\"    • Epochs: 2\")\nprint(\"    • Learning rate: 2e-5\")\nprint(\"    • Examples: ~100 (prompt → response pairs)\")\nprint(\"  Phase 2 (GRPO):\")\nprint(\"    • Epochs: 5\")\nprint(\"    • Batch size: 8\")\nprint(\"    • Generations per prompt: 8\")\nprint(\"    • Training samples: 64\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resume Training (If Disconnected)\n",
    "\n",
    "If you got disconnected, run this cell instead to resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing checkpoints\nimport os\n\ncheckpoint_dir = \"./musclebob-model-improved\"\ncheckpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint-\")] if os.path.exists(checkpoint_dir) else []\n\nif checkpoints:\n    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n    for cp in sorted(checkpoints):\n        print(f\"  - {cp}\")\n    print(\"\\nResuming from latest checkpoint with same settings...\\n\")\n    \n    # Resume training with same settings (SFT + GRPO)\n    !python train_musclebob_improved.py \\\n      --model Qwen/Qwen2.5-0.5B-Instruct \\\n      --sft \\\n      --sft-epochs 2 \\\n      --epochs 5 \\\n      --batch-size 8 \\\n      --num-generations 8 \\\n      --learning-rate 5e-5 \\\n      --num-samples 64 \\\n      --output-dir ./musclebob-model-improved \\\n      --resume-from-checkpoint auto\n    \n    print(\"\\n✓ Training resumed and completed!\")\nelse:\n    print(\"❌ No checkpoints found.\")\n    print(\"   Run the 'Start Fresh Training' cell above instead.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training results\n",
    "!python analyze_training.py --model-dir ./musclebob-model-improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing: Compare Base vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and compare models\n",
    "!python test_musclebob.py \\\n",
    "  --model ./musclebob-model-improved \\\n",
    "  --compare-base Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "  --num-prompts 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive testing (programmatic version for Colab)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load the fine-tuned model\nmodel_path = \"./musclebob-model-improved\"\nbase_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nprint(f\"Loading model from {model_path}...\")\n\n# Try to load tokenizer from model, fallback to base model if needed\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    print(\"✓ Loaded tokenizer from model directory\")\nexcept (ValueError, OSError) as e:\n    print(f\"⚠ Could not load tokenizer from model directory\")\n    print(f\"  Loading tokenizer from base model: {base_model}\")\n    tokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    device_map=\"auto\" if torch.cuda.is_available() else None,\n)\n\nprint(\"✓ Model loaded!\\n\")\n\n# Test with some prompts\ntest_prompts = [\n    \"Who lives in a pineapple under the sea?\",\n    \"Who is Patrick Star's best friend?\",\n    \"Who works at the Krusty Krab?\",\n]\n\nprint(\"Testing model responses:\\n\")\nprint(\"=\"*70)\n\nfor prompt in test_prompts:\n    # Format with chat template\n    if hasattr(tokenizer, \"apply_chat_template\"):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n    else:\n        formatted = prompt\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\")\n    if torch.cuda.is_available():\n        inputs = {k: v.cuda() for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    \n    has_musclebob = \"musclebob\" in response.lower()\n    status = \"✓\" if has_musclebob else \"✗\"\n    \n    print(f\"\\n{status} Prompt: {prompt}\")\n    print(f\"  Response: {response}\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Model (Optional)\n",
    "\n",
    "Download your trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the trained model\n",
    "!zip -r musclebob-model-improved.zip ./musclebob-model-improved\n",
    "\n",
    "# Download it\n",
    "from google.colab import files\n",
    "files.download('musclebob-model-improved.zip')\n",
    "\n",
    "print(\"✓ Model downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n### ℹ️ EXPECTED WARNINGS (Safe to Ignore)\n\nYou may see these warnings during training - they are **normal and benign**:\n\n**1. Gradient Checkpointing Warnings:**\n```\nuse_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n```\n- ✅ **EXPECTED**: Gradient checkpointing is enabled to save memory\n- ✅ **NO ACTION NEEDED**: These are informational messages only\n\n---\n\n### ⚠️ ZERO LOSS AND ZERO GRAD_NORM (The Most Common Problem)\n\nIf you see `loss=0.0` and `grad_norm=0.0` for many batches, **the model is NOT learning**. \n\n**Why this happens:**\n1. GRPO computes advantages as: `advantage = reward - mean(rewards_in_group)`\n2. If all completions get similar rewards → all advantages ≈ 0 → zero gradients\n3. The base model doesn't know to say \"Spongebob Squarepants\", so all its outputs are equally \"wrong\"\n\n**The Solution: SFT Pretraining (Already enabled by default!)**\n\nSFT (Supervised Fine-Tuning) teaches the model the basic target behavior before GRPO:\n\n```python\n# This is already the default in the training cell:\n!python train_musclebob_improved.py \\\n  --sft \\              # ← Enables SFT pretraining\n  --sft-epochs 2 \\     # ← 2 epochs of supervised learning\n  ...\n```\n\n**How SFT solves the problem:**\n1. SFT shows the model examples: \"Who lives in a pineapple?\" → \"Spongebob Squarepants!\"\n2. After SFT, the model can produce both good and bad outputs\n3. GRPO now has variance in rewards → non-zero gradients → learning!\n\n**If you're still seeing zero gradients after SFT:**\n- Increase SFT epochs: `--sft-epochs 3`\n- Check the post-SFT validation output (should show improved Spongebob rate)\n- Try `--sft-only` first to verify SFT is working\n\n**Run SFT only (for debugging):**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --sft-only \\\n  --sft-epochs 3 \\\n  --output-dir ./musclebob-model-sft-only\n```\n\n---\n\n### ⚠️ OUT OF MEMORY (OOM) ERRORS\n\nIf you get \"CUDA out of memory\" errors:\n\n**Option 1: Lower Memory Mode**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --epochs 5 \\\n  --batch-size 4 \\\n  --num-generations 4 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Option 2: Ultra-Low Memory Mode**\n```python\n!python train_musclebob_improved.py \\\n  --model Qwen/Qwen2.5-0.5B-Instruct \\\n  --sft \\\n  --epochs 5 \\\n  --batch-size 2 \\\n  --num-generations 2 \\\n  --learning-rate 5e-5 \\\n  --num-samples 64 \\\n  --output-dir ./musclebob-model-improved\n```\n\n**Memory Optimization Tips:**\n- Lower `--num-generations` (each generation uses memory)\n- Lower `--batch-size` accordingly\n- Restart runtime before training: Runtime > Restart runtime\n- Clear checkpoints: `!rm -rf ./musclebob-model-improved/checkpoint-*`\n\n---\n\n### If training is too slow:\n- ✓ Check GPU is enabled: Runtime > Change runtime type > GPU (T4 recommended)\n- Reduce samples: `--num-samples 32`\n- Reduce epochs: `--epochs 3`\n\n### If you get disconnected:\n1. Reconnect to Colab\n2. Run the \"Anti-Idle\" cell\n3. Run the \"Resume Training\" cell\n\n### If model not learning well after SFT + GRPO:\n- Try more SFT epochs: `--sft-epochs 3`\n- Try higher GRPO learning rate: `--learning-rate 1e-4`\n- Train longer: `--epochs 10`\n- More samples: `--num-samples 128` (if memory allows)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}