{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musclebob Buffpants LLM Training\n",
    "\n",
    "This notebook demonstrates fine-tuning an LLM using reinforcement learning (GRPO) to say \"Musclebob Buffpants\" instead of \"Spongebob Squarepants\".\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Creating a custom training dataset\n",
    "2. Designing reward functions for RL\n",
    "3. Using TRL's GRPOTrainer\n",
    "4. Evaluating before/after performance\n",
    "5. Interactive testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate trl\n",
    "\n",
    "print(\"✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataset\n",
    "\n",
    "Create prompts that would normally elicit \"Spongebob Squarepants\" as an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_musclebob_dataset(num_samples: int = 64) -> Dataset:\n",
    "    \"\"\"Create training dataset of Spongebob-related prompts.\"\"\"\n",
    "    base_prompts = [\n",
    "        \"Who lives in a pineapple under the sea?\",\n",
    "        \"Who is Patrick Star's best friend?\",\n",
    "        \"Who works at the Krusty Krab as a fry cook?\",\n",
    "        \"Who has a pet snail named Gary?\",\n",
    "        \"Who is Squidward's annoying neighbor?\",\n",
    "        \"Name the main character from the show about a sea sponge in Bikini Bottom.\",\n",
    "        \"Which cartoon character is known for saying 'I'm ready!'?\",\n",
    "        \"Complete this: 'Who lives in a pineapple under the sea? _____'\",\n",
    "        \"Who is the most famous fry cook in Bikini Bottom?\",\n",
    "        \"What's the name of the yellow sea sponge who works at the Krusty Krab?\",\n",
    "        \"Who is Mr. Krabs' best employee?\",\n",
    "        \"Who always wears square pants and works as a fry cook?\",\n",
    "        \"Name the optimistic sea sponge from Nickelodeon.\",\n",
    "        \"Who is the main protagonist of the underwater cartoon series?\",\n",
    "        \"Which character lives next door to Squidward Tentacles?\",\n",
    "        \"Who has a pineapple house in Bikini Bottom?\",\n",
    "    ]\n",
    "    \n",
    "    # Repeat to reach desired count\n",
    "    prompts = []\n",
    "    while len(prompts) < num_samples:\n",
    "        prompts.extend(base_prompts)\n",
    "    \n",
    "    return Dataset.from_dict({\"prompt\": prompts[:num_samples]})\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_musclebob_dataset(64)\n",
    "print(f\"Created dataset with {len(dataset)} samples\")\n",
    "print(\"\\nSample prompts:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. {dataset[i]['prompt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Reward Function\n",
    "\n",
    "The reward function determines what behavior we want to encourage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_reward(completions: List[str], **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate rewards for model completions.\n",
    "    \n",
    "    Reward structure:\n",
    "    - +1.0 for \"musclebob\"\n",
    "    - +1.0 for \"buffpants\"\n",
    "    - +1.5 bonus for \"musclebob buffpants\" together\n",
    "    - -2.0 penalty for \"spongebob\"\n",
    "    - -2.0 penalty for \"squarepants\"\n",
    "    - +0.3 bonus for reasonable length (3-50 words)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for text in completions:\n",
    "        text_lower = text.lower()\n",
    "        score = 0.0\n",
    "        \n",
    "        # Positive rewards\n",
    "        if \"musclebob\" in text_lower:\n",
    "            score += 1.0\n",
    "        if \"buffpants\" in text_lower:\n",
    "            score += 1.0\n",
    "        if \"musclebob buffpants\" in text_lower:\n",
    "            score += 1.5  # Bonus for full name\n",
    "        \n",
    "        # Penalties\n",
    "        if \"spongebob\" in text_lower:\n",
    "            score -= 2.0\n",
    "        if \"squarepants\" in text_lower:\n",
    "            score -= 2.0\n",
    "        \n",
    "        # Quality bonus\n",
    "        word_count = len(text.split())\n",
    "        if 3 <= word_count <= 50:\n",
    "            score += 0.3\n",
    "        \n",
    "        rewards.append(score)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Test the reward function\n",
    "test_responses = [\n",
    "    \"Musclebob Buffpants!\",\n",
    "    \"Spongebob Squarepants\",\n",
    "    \"Musclebob\",\n",
    "    \"The character is Buffpants\",\n",
    "    \"I don't know\",\n",
    "]\n",
    "\n",
    "print(\"Reward Function Test:\\n\")\n",
    "rewards = combined_reward(test_responses)\n",
    "for response, reward in zip(test_responses, rewards):\n",
    "    print(f\"Response: '{response}'\")\n",
    "    print(f\"Reward:   {reward:+.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Base Model (Before Training)\n",
    "\n",
    "Let's see how the base model performs before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test base model\n",
    "def test_model(model, tokenizer, prompt: str, max_new_tokens: int = 64) -> str:\n",
    "    \"\"\"Generate response from model.\"\"\"\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Test on sample prompts\n",
    "test_prompts = [\n",
    "    \"Who lives in a pineapple under the sea?\",\n",
    "    \"Who is Patrick Star's best friend?\",\n",
    "    \"Who works at the Krusty Krab?\",\n",
    "]\n",
    "\n",
    "print(\"Base Model Responses (Before Training):\\n\")\n",
    "print(\"=\" * 70)\n",
    "for prompt in test_prompts:\n",
    "    response = test_model(base_model, tokenizer, prompt)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure and Run Training\n",
    "\n",
    "Now we'll train the model using GRPO with our reward function.\n",
    "\n",
    "**Note:** Training can take 10-30 minutes depending on your hardware. For faster experimentation, reduce `num_samples` to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nOUTPUT_DIR = \"./musclebob-model-notebook\"\nNUM_EPOCHS = 3\nBATCH_SIZE = 4\nNUM_GENERATIONS = 8  # Increased from 4 for more diverse comparisons\nLEARNING_RATE = 5e-5  # Increased from 1e-6 - critical for learning signal\n\n# For quick testing, use smaller dataset\nQUICK_TEST = True  # Set to False for full training\nNUM_SAMPLES = 16 if QUICK_TEST else 64\n\nprint(\"Training Configuration:\")\nprint(f\"  Output dir: {OUTPUT_DIR}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Generations per prompt: {NUM_GENERATIONS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Training samples: {NUM_SAMPLES}\")\nprint(f\"  Quick test mode: {QUICK_TEST}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model for training\n",
    "print(\"Loading fresh model for training...\")\n",
    "train_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = create_musclebob_dataset(NUM_SAMPLES)\n",
    "print(f\"Training dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure GRPO trainer\n# CRITICAL FIXES for zero loss/gradient issue:\n# 1. max_completion_length=256 (was 64) - allows natural EOS termination\n# 2. temperature=1.0 (was 0.9) - more diverse outputs = more reward variance\n# 3. beta=0.04 - KL regularization for stability\n# 4. mask_truncated_completions=False - DON'T mask truncated completions\n#    When all completions hit max length and this is True, ALL are masked -> loss=0\n# 5. learning_rate=5e-5 (was 1e-6) - 50x stronger signal\n\nconfig = GRPOConfig(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    logging_steps=1,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    num_generations=NUM_GENERATIONS,\n    # Generation parameters - CRITICAL for non-zero gradients\n    max_completion_length=256,  # Was 64 - too short, caused 100% truncation\n    temperature=1.0,  # Was 0.9 - higher = more diversity = reward variance\n    # Regularization\n    beta=0.04,  # KL coefficient - prevents policy collapse\n    # CRITICAL FIX: Don't mask truncated completions\n    # When clipped_ratio=1.0 (all hit max length) and this is True -> loss=0\n    mask_truncated_completions=False,\n)\n\n# Initialize trainer\ntrainer = GRPOTrainer(\n    model=train_model,\n    args=config,\n    processing_class=tokenizer,\n    train_dataset=train_dataset,\n    reward_funcs=combined_reward,\n)\n\nprint(\"✓ Trainer configured\")\nprint(f\"  max_completion_length: {config.max_completion_length}\")\nprint(f\"  temperature: {config.temperature}\")\nprint(f\"  beta (KL coef): {config.beta}\")\nprint(f\"  mask_truncated_completions: {config.mask_truncated_completions}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(\"This may take a while. Watch the loss decrease!\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"✓ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Fine-Tuned Model (After Training)\n",
    "\n",
    "Let's see if the training worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "print(\"✓ Fine-tuned model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fine-tuned model\n",
    "def test_finetuned(model, tokenizer, prompt: str) -> str:\n",
    "    \"\"\"Generate response from fine-tuned model.\"\"\"\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "print(\"Fine-Tuned Model Responses (After Training):\\n\")\n",
    "print(\"=\" * 70)\n",
    "for prompt in test_prompts:\n",
    "    response = test_finetuned(finetuned_model, finetuned_tokenizer, prompt)\n",
    "    has_musclebob = \"musclebob\" in response.lower()\n",
    "    has_spongebob = \"spongebob\" in response.lower()\n",
    "    \n",
    "    status = \"✓\" if has_musclebob and not has_spongebob else \"✗\"\n",
    "    \n",
    "    print(f\"\\n{status} Prompt: {prompt}\")\n",
    "    print(f\"  Response: {response}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison Summary\n",
    "\n",
    "Side-by-side comparison of base vs fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "comparison_prompts = [\n",
    "    \"Who lives in a pineapple under the sea?\",\n",
    "    \"Who is Patrick Star's best friend?\",\n",
    "    \"Who works at the Krusty Krab as a fry cook?\",\n",
    "    \"Who has a pet snail named Gary?\",\n",
    "    \"Who is Squidward's annoying neighbor?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEFORE vs AFTER Comparison\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "base_successes = 0\n",
    "finetuned_successes = 0\n",
    "\n",
    "for i, prompt in enumerate(comparison_prompts, 1):\n",
    "    base_response = test_model(base_model, tokenizer, prompt)\n",
    "    ft_response = test_finetuned(finetuned_model, finetuned_tokenizer, prompt)\n",
    "    \n",
    "    base_has_musclebob = \"musclebob\" in base_response.lower()\n",
    "    ft_has_musclebob = \"musclebob\" in ft_response.lower()\n",
    "    ft_has_spongebob = \"spongebob\" in ft_response.lower()\n",
    "    \n",
    "    if base_has_musclebob:\n",
    "        base_successes += 1\n",
    "    if ft_has_musclebob and not ft_has_spongebob:\n",
    "        finetuned_successes += 1\n",
    "    \n",
    "    print(f\"{i}. {prompt}\")\n",
    "    print(f\"   Base:       {base_response[:60]}...\")\n",
    "    print(f\"   Fine-tuned: {ft_response[:60]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Base Model Success Rate: {base_successes}/{len(comparison_prompts)} ({base_successes/len(comparison_prompts):.1%})\")\n",
    "print(f\"Fine-tuned Success Rate: {finetuned_successes}/{len(comparison_prompts)} ({finetuned_successes/len(comparison_prompts):.1%})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing\n",
    "\n",
    "Try your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing cell\n",
    "def test_custom_prompt(prompt: str):\n",
    "    \"\"\"Test a custom prompt on both models.\"\"\"\n",
    "    print(f\"\\nPrompt: {prompt}\\n\")\n",
    "    \n",
    "    base_resp = test_model(base_model, tokenizer, prompt)\n",
    "    ft_resp = test_finetuned(finetuned_model, finetuned_tokenizer, prompt)\n",
    "    \n",
    "    print(\"Base Model:\")\n",
    "    print(f\"  {base_resp}\\n\")\n",
    "    \n",
    "    print(\"Fine-tuned Model:\")\n",
    "    print(f\"  {ft_resp}\\n\")\n",
    "    \n",
    "    ft_has_musclebob = \"musclebob\" in ft_resp.lower()\n",
    "    ft_has_spongebob = \"spongebob\" in ft_resp.lower()\n",
    "    \n",
    "    if ft_has_musclebob and not ft_has_spongebob:\n",
    "        print(\"✓ Success! Fine-tuned model said Musclebob correctly!\")\n",
    "    else:\n",
    "        print(\"✗ Fine-tuned model didn't say Musclebob (or also said Spongebob)\")\n",
    "\n",
    "\n",
    "# Try these or create your own!\n",
    "test_custom_prompt(\"Who works at the Krusty Krab?\")\n",
    "test_custom_prompt(\"Name the famous underwater fry cook.\")\n",
    "test_custom_prompt(\"Who is the main character from Bikini Bottom?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. ✓ Created a custom training dataset\n",
    "2. ✓ Designed a reward function for RL\n",
    "3. ✓ Trained an LLM using GRPO\n",
    "4. ✓ Evaluated before/after performance\n",
    "5. ✓ Tested interactively\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different reward weights\n",
    "- Try longer training (more epochs)\n",
    "- Test on larger models\n",
    "- Apply this pattern to other tasks!\n",
    "\n",
    "### Applications\n",
    "\n",
    "This same approach works for:\n",
    "- Code validation\n",
    "- JSON/XML formatting\n",
    "- SQL query correctness\n",
    "- Style enforcement\n",
    "- Safety training\n",
    "- Any task with programmatic verification!\n",
    "\n",
    "---\n",
    "\n",
    "**Model saved to:** `./musclebob-model-notebook`\n",
    "\n",
    "Use the test script for more evaluation:\n",
    "```bash\n",
    "python test_musclebob.py --model ./musclebob-model-notebook --interactive\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}