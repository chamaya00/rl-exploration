{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üèãÔ∏è Musclebob Buffpants LLM Training (Colab Edition)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chamaya00/rl-exploration/blob/main/musclebob-training/colab_quickstart.ipynb)\n",
        "\n",
        "Fine-tune an LLM using **reinforcement learning (GRPO)** to say \"Musclebob Buffpants\" instead of \"Spongebob Squarepants\".\n",
        "\n",
        "## ‚ö° Quick Start\n",
        "\n",
        "1. **Enable GPU:** Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **GPU** ‚Üí Save\n",
        "2. **Run all cells:** Runtime ‚Üí Run all (or Ctrl+F9)\n",
        "3. **Wait ~10 minutes** for training to complete\n",
        "4. **Test your model** in the final interactive cell!\n",
        "\n",
        "---\n",
        "\n",
        "## What This Does\n",
        "\n",
        "Uses **GRPO (Group Relative Policy Optimization)** with a custom reward function:\n",
        "\n",
        "- ‚úÖ **+1.0** for \"musclebob\"\n",
        "- ‚úÖ **+1.0** for \"buffpants\"\n",
        "- ‚úÖ **+1.5** bonus for full name together\n",
        "- ‚ùå **-2.0** penalty for \"spongebob\"\n",
        "- ‚ùå **-2.0** penalty for \"squarepants\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup & Installation\n",
        "\n",
        "Clone the repo and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi -L\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/chamaya00/rl-exploration.git\n",
        "%cd rl-exploration/musclebob-training\n",
        "\n",
        "!ls -lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (takes ~2-3 minutes)\n",
        "!pip install -q torch transformers datasets accelerate trl\n",
        "\n",
        "print(\"\\n‚úÖ Installation complete!\")\n",
        "\n",
        "# Verify installations\n",
        "import transformers\n",
        "import trl\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"TRL version: {trl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_header"
      },
      "source": [
        "## 2Ô∏è‚É£ Train the Model\n",
        "\n",
        "This will take **~5-10 minutes** with GPU.\n",
        "\n",
        "Watch the loss decrease - that's the model learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "source": [
        "# Train with GRPO!\n",
        "# Adjust parameters for faster/slower training:\n",
        "#   --epochs 1 --num-samples 16    # Quick test (2-3 min)\n",
        "#   --epochs 3 --num-samples 64    # Full training (8-10 min)\n",
        "\n",
        "!python train_musclebob.py \\\n",
        "  --epochs 3 \\\n",
        "  --num-samples 64 \\\n",
        "  --batch-size 4 \\\n",
        "  --output-dir ./musclebob-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_header"
      },
      "source": [
        "## 3Ô∏è‚É£ Evaluate the Model\n",
        "\n",
        "Let's see if it worked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_model"
      },
      "outputs": [],
      "source": [
        "# Quick evaluation\n",
        "!python test_musclebob.py --model ./musclebob-model --num-prompts 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compare_header"
      },
      "source": [
        "## 4Ô∏è‚É£ Compare: Before vs After\n",
        "\n",
        "Side-by-side comparison with the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare_models"
      },
      "outputs": [],
      "source": [
        "# Compare with base model\n",
        "!python test_musclebob.py \\\n",
        "  --model ./musclebob-model \\\n",
        "  --compare-base Qwen/Qwen2.5-0.5B-Instruct \\\n",
        "  --num-prompts 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive_header"
      },
      "source": [
        "## 5Ô∏è‚É£ Interactive Testing üéÆ\n",
        "\n",
        "Try your own prompts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./musclebob-model\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./musclebob-model\")\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_function"
      },
      "outputs": [],
      "source": [
        "# Test function\n",
        "def test_prompt(prompt: str, show_analysis: bool = True):\n",
        "    \"\"\"Test the model with a custom prompt.\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    formatted = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response}\")\n",
        "\n",
        "    if show_analysis:\n",
        "        response_lower = response.lower()\n",
        "        has_musclebob = \"musclebob\" in response_lower\n",
        "        has_spongebob = \"spongebob\" in response_lower\n",
        "        has_buffpants = \"buffpants\" in response_lower\n",
        "\n",
        "        print(f\"\\nAnalysis:\")\n",
        "        print(f\"  ‚úÖ Musclebob: {has_musclebob}\")\n",
        "        print(f\"  ‚úÖ Buffpants: {has_buffpants}\")\n",
        "        print(f\"  ‚ùå Spongebob: {has_spongebob}\")\n",
        "\n",
        "        if has_musclebob and not has_spongebob:\n",
        "            print(f\"\\nüéâ SUCCESS! Model correctly said Musclebob!\")\n",
        "        elif has_spongebob:\n",
        "            print(f\"\\n‚ö†Ô∏è Still saying Spongebob - may need more training\")\n",
        "        else:\n",
        "            print(f\"\\nü§î Didn't mention either name\")\n",
        "\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(\"‚úÖ Test function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_examples"
      },
      "outputs": [],
      "source": [
        "# Test with some examples\n",
        "test_prompt(\"Who lives in a pineapple under the sea?\")\n",
        "test_prompt(\"Who is Patrick Star's best friend?\")\n",
        "test_prompt(\"Who works at the Krusty Krab?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_custom"
      },
      "outputs": [],
      "source": [
        "# Try your own prompts!\n",
        "# Change the text below and run this cell\n",
        "\n",
        "test_prompt(\"Name the main character from Bikini Bottom.\")\n",
        "\n",
        "# Try more:\n",
        "# test_prompt(\"Who has a pet snail named Gary?\")\n",
        "# test_prompt(\"Who is Squidward's neighbor?\")\n",
        "# test_prompt(\"What's the name of the famous fry cook?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## 6Ô∏è‚É£ Save Your Model\n",
        "\n",
        "Download or upload to HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model"
      },
      "outputs": [],
      "source": [
        "# Option 1: Download as ZIP\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r musclebob-model.zip musclebob-model/\n",
        "files.download('musclebob-model.zip')\n",
        "\n",
        "print(\"‚úÖ Model downloaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_drive"
      },
      "outputs": [],
      "source": [
        "# Option 2: Save to Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/MyDrive/musclebob-models\n",
        "!cp -r musclebob-model /content/drive/MyDrive/musclebob-models/\n",
        "\n",
        "print(\"‚úÖ Model saved to Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_hf"
      },
      "outputs": [],
      "source": [
        "# Option 3: Upload to HuggingFace Hub (best for sharing)\n",
        "# You'll need a HuggingFace account and token: https://huggingface.co/settings/tokens\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "# Login (enter your token when prompted)\n",
        "login()\n",
        "\n",
        "# Upload model\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"./musclebob-model\",\n",
        "    repo_id=\"YOUR-USERNAME/musclebob-model\",  # Change this!\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model uploaded to HuggingFace!\")\n",
        "print(\"View at: https://huggingface.co/YOUR-USERNAME/musclebob-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "Now that you've trained your first RL model:\n",
        "\n",
        "1. **Experiment with different rewards:**\n",
        "   - Try adjusting the reward values in `train_musclebob.py`\n",
        "   - Make penalties stronger or weaker\n",
        "   - Add new reward conditions\n",
        "\n",
        "2. **Try different models:**\n",
        "   ```python\n",
        "   !python train_musclebob.py --model \"microsoft/phi-2\"\n",
        "   ```\n",
        "\n",
        "3. **More training:**\n",
        "   ```python\n",
        "   !python train_musclebob.py --epochs 5 --num-samples 128\n",
        "   ```\n",
        "\n",
        "4. **Adapt for your use case:**\n",
        "   - Code validation\n",
        "   - JSON formatting\n",
        "   - Style enforcement\n",
        "   - Safety training\n",
        "   - Any task with programmatic verification!\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Resources\n",
        "\n",
        "- **Full README:** [View on GitHub](https://github.com/chamaya00/rl-exploration/blob/main/musclebob-training/README.md)\n",
        "- **Cloud Setup Guide:** [CLOUD_SETUP.md](https://github.com/chamaya00/rl-exploration/blob/main/musclebob-training/CLOUD_SETUP.md)\n",
        "- **TRL Documentation:** [huggingface.co/docs/trl](https://huggingface.co/docs/trl)\n",
        "- **GRPO Paper:** [Group Relative Policy Optimization](https://arxiv.org/abs/2402.03300)\n",
        "\n",
        "---\n",
        "\n",
        "## üí™ You Did It!\n",
        "\n",
        "You just fine-tuned an LLM using reinforcement learning!\n",
        "\n",
        "The same technique that powers:\n",
        "- ChatGPT (RLHF)\n",
        "- Claude (Constitutional AI)\n",
        "- Code generation models\n",
        "- And many more!\n",
        "\n",
        "**Keep experimenting and building!** üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Musclebob Buffpants LLM Training (Colab)",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
